---
layout: post
title:  "无人驾驶第一课-Lesson 4：感知"
date:   2018-07-18 21:08:31
categories: self-driving
tags: 无人驾驶
      Apollo
---

* content
{:toc}

从 Apollo 起步-Lesson 4：感知
<!--more-->


# 感知
在本课中 我们将首先介绍计算机视觉的基本应用领域，然后我们将介绍机器学习、神经网络和卷积神经网络的基础知识。

我们将继续讨论感知模块在无人驾驶车中的具体任务，接下来将介绍 Apollo 感知模块的体系结构和传感器融合的相关主题。希望这会让你对无人驾驶感知系统有一个清晰认识

## 计算机视觉
作为人类 你和我可以自动识别图像中的物体，甚至可以推断这些物体之间的关系。但是 对于计算机而言，图像只是红色、绿色和蓝色值的集合。如何将这些颜色值翻译为解读有意义的图像内容并不明显。

**无人驾驶车有四个感知世界的核心任务**。
- 检测 是指找出物体在环境中的位置

- 分类 是指明确对象是什么

- 跟踪 是指随时间的推移观察移动物体。如其他车辆、自行车和行人
- 语义分割意味着将图像中的每个像素与语义类别进行匹配。如道路、汽车或天空

![](http://p5ocy6pck.bkt.clouddn.com/perception.jpg)

我们可将 **分类** 作为作为研究计算机视觉一般数据流程的例子。

**图像分类器是一种将图像作为输入并输出标识该图像的标签或“类别”的算法**。例如 交通标志分类器查看停车标志并识别它是停车标志、让路标志、限速标志，还是其他类型的标志。分类器甚至可以识别行为。比如一个人是在走路 还是在跑步。

![](http://p5ocy6pck.bkt.clouddn.com/classification%20steps.jpg)

分类器有很多种，但它们都包含一系列类似的步骤。
- 首先 计算机接收类似摄像头等成像设备的输入，这通常被捕获为图像或一系列图像。
- 然后通过预处理发送每个图像预处理对每个图像进行了标准化处理。常见的预处理步骤包括调整图像大小或旋转图像或将图像从一个色彩空间转换为另一个色彩空间。例如从全彩到灰度。预处理可帮助我们的模型更快地处理和学习图像。
- 接下来 提取特征。特征有助于计算机理解图像。例如 将汽车与自行车区分开来的一些特征，汽车通常具有更大的形状 并且它有四个轮子而不是两个，形状和车轮将是汽车的显著特征。我们将在本课的后面详细讨论特征。
- 最后 这些特征被输入到分类模型中，此步骤使用特征来选择图像类别。例如 分类器可以确定图像是否包含汽车、自行车、行人或者根本不包含这样的对象。为了完成这些视觉任务，需要建立模型。模型是帮助计算机了解图像内容的工具，在计算机视觉中 无论经过训练的模型执行什么任务，它们通常在开始时将摄像头图像作为输入。

## 摄像头图像

摄像头图像是最常见的计算机视觉数据。

以这张汽车照片为例，让我们看看计算机如何认为这实际上是一辆汽车的图像。

![](http://p5ocy6pck.bkt.clouddn.com/%E5%9B%BE%E5%83%8F%E7%9F%A9%E9%98%B5.jpg)

 **从计算机的角度来看，图像只是一个二维网格，也被称为矩阵**。矩阵中的每个单元格都包含一个值，数字图像全部由像素组成，其中包含非常小的颜色或强度单位。

图像中的每个像素都只是一个数值，这些值构成了我们的图像矩阵。我们甚至可以改变这些像素值。我们可以通过为每个像素值添加一个标量整数来改变图像亮度，我们也可以向右移动每个像素值，我们还可以执行许多其他操作。通常 这些数字网格是许多图像处理技术的基础。多数颜色和形状转换都只是通过对图像进行数学运算以及逐一像素进行更改来完成。现在是一个将图像分解为二维灰度像素值网格的示例。

![](http://p5ocy6pck.bkt.clouddn.com/RGB%E5%9B%BE.jpg)

彩色图像是相似的 但更复杂一点，彩色图像被构建为值的三维立方体。每个立方体都有高度、宽度和深度，深度为颜色通道数量。大多数彩色图像以三种颜色组合表示，红色、绿色和蓝色，这些图像被称为 **RGB 图像**。对于 RGB 图像 深度为 3！因此 RGB 图像可以用一个薄盒子表示，将深度视为三重叠加的二维色层很有帮助。一层为红色，一层为绿色、一层为蓝色，它们一起构建了一个完整的彩色图像。

## LiDAR 图像

激光雷达传感器创建环境的点云表征，提供了难以通过摄像头图像获得的信息，如距离和高度。

激光雷达传感器使用光线 ，尤其是激光 来测量与环境中反射该光线的物体之间的距离。激光雷达发射激光脉冲并测量物体将每个激光脉冲反射回传感器所花费的时间。反射需要的时间越长 物体离传感器越远，激光雷达正是通过这种方式来构建世界的视觉表征。

![](http://p5ocy6pck.bkt.clouddn.com/lidar%E5%9B%BE%E5%83%8F.jpg)

你可以在此可视化视图中看到激光雷达的输出，激光雷达通过发射光脉冲来检测汽车周围的环境。蓝色点表示反射激光脉冲的物体，中间的黑色区域是无人驾驶车本身占据的空间。由于激光雷达测量激光束反射，它收集的数据形成一团点或 **“点云”**。

点云中的每个点代表反射回传感器的激光束，这些点云可以告诉我们关于物体的许多信息，例如其形状和表面纹理。通过对点进行聚类和分析，这些数据提供了足够的对象检测、跟踪或分类信息，在这里你可以看到在点云上执行的检测和分类结果。红点为行人 绿点表示其他汽车，正如你所看到的那样 激光雷达数据提供了用于构建世界视觉表征的足够空间信息。

计算机视觉技术不仅可以使用摄像头图像进行对象分类，还可以使用点云和其他类型的空间相关数据进行对象分类。

## 机器学习

机器学习是使用特殊算法来训练计算机从数据中学习的计算机科学领域。通常 这种学习结果存放在一种被称为“模型”的数据结构中。有很多种模型，事实上 “模型”只是一种可用于理解和预测世界的数据结构。

- 机器学习涉及使用数据和相关的真值标记来进行模型训练。例如 可能会显示车辆和行人的计算机图像，以及告诉计算机哪个是哪个的标签，我会让计算机学习如何最好地区分两类图像，这类机器学习也称为 **监督式学习**，因为模型利用了人类创造的真值标记

- 你可以设想一个类似的学习过程，但这次使用的是没有真值标记的车辆与行人图像。在这种方法中 我们会让计算机自行决定，哪些图像相似 哪些图像不同，这被称为 **无监督学习**。我们在这不提供真值标记，而是通过分析输入的数据，计算机凭借自行学习找到区别。
- 另一种方法被称为 **“半监督式”学习**。它将监督学习和无监督学习的特点结合在一起，该方法使用少量的标记数据和大量的未标记数据来训练模型。

- **强化学习**  是另一种机器学习，强化学习涉及允许模型通过尝试许多不同的方法来解决问题，然后衡量哪种方法最为成功。计算机将尝试许多不同的解决方案，最终使其方法与环境相适应。

![](http://p5ocy6pck.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BB%83%E4%B9%A0%E9%A2%98.jpg)

例如 在模拟器中，强化学习智能体可训练汽车进行右转，智能体将在初始位置发动车辆，然后进行实验性驾驶，以多种不同的方向和速度。如果汽车实际完成了右转，智能体会提高奖励，即得分，这是针对导致成功结果的初始操作。起初 汽车可能无法找到执行转弯的方法，然而 就像人类那样，汽车最终会从一些成功的右转经验中学习，最后学会如何完成任务。

## 神经网络

人工神经网络是通过数据来学习复杂模式的工具。神经网络由大量的神经元组成，正如人体神经系统的神经元那样，人工神经元负责传递和处理信息，也可以对这些神经元进行训练。

你可以将一些图像识别为车辆 无论它们是黑是白或大或小，你甚至可能不知道自己如何知道它们是车辆，也许是某些特征触发了你的反应，如车轮、车灯和车窗。

人工神经网络具有相似的运作方式，通过密集训练 计算机可以辨别汽车、行人、交通信号灯和电线杆。

![](http://p5ocy6pck.bkt.clouddn.com/how%20know%20a%20car.jpg)

当看到该图像时，你的大脑如何工作？你的大脑可能会将图像分为几部分，然后识别特征，如车轮、车窗和颜色 然后 大脑将使用这些特征对图像进行检测和分类。例如 在确定图像是否为汽车时，大脑可能不会认为颜色是关键特征 因为汽车有多种颜色，所以大脑会将更多权重放在其他特征上 并降低颜色的重要性。

同样地 **神经网络也会从图像中提取许多特征**但这些特征可能是我们人类无法描述或甚至无法理解的特征，但我们最终并不需要理解，计算机将 **调整这些特征的权重**以完成神经网络的最终任务，这就是深层神经网络的思维方式。

## 反向传播算法

我们已经讨论过神经网络如何从数据中“学习”,那么你可能想知道这种学习如何发生。

**学习有时称为训练，它由三步循环组成：前馈、误差测定和反向传播**。

- 首先随机分配初始权重，即人工神经元的值。通过神经网络来馈送每个图像 产生输出值，这被称为前馈。

![](http://p5ocy6pck.bkt.clouddn.com/backpropagation.jpg)

- 下一步为误差测定，误差是真值标记与与前馈过程所产生输出之间的偏差。
- 最后一步是反向传播，通过神经网络反向发送误差此过程类似前馈过程 只是以相反方向进行。

每个人工神经元都对其值进行微调，这是基于通过神经网络后向传播的误差。所有这些独立调整的结果 可生成更准确的网络 。

一个训练周期： 包括前馈、误差测定和反向传播还远远不够，为了训练网络 通常需要数千个这样的周期。但最终结果应该是：模型能够根据新数据做出准确预测。

## 卷积神经网络

卷积神经网络 （CNN） 是 一种人工神经网络， 它对感知问题特别有效。CNN 接受多维输入，包括定义大多数传感器数据的二维和三维形状。

如果使用标准神经网络对图像进行分类，则需要通过一种方法将图像连接到网络的第一层，这属于一维。标准做法是通过将图像矩阵重塑为一个矢量，并在一个大行中连接所有列 将图像“展开”为一维像素阵列 。

![](http://p5ocy6pck.bkt.clouddn.com/%E4%B8%80%E7%BB%B4.jpg)

然而 这种方法打破了图像中所嵌入的空间信息，如果图像中有车轮，则车轮中的所有像素将散布在整个像素阵列中。但我们知道 这些像素，以二维方式连接形成车轮。如果我们将其散布在一个维度上，神经网络很难从图像中提取车轮。

CNN 通过维持输入像素之间的空间关系来解决这个问题。具体来说 CNN 通过将过滤器连续滑过图像来收集信息，每次收集信息时，只对整个图像的一小部分区域进行分析，这被称为 “**卷积**”。

![](http://p5ocy6pck.bkt.clouddn.com/cnn.jpg)
当我们在整个输入图像上对一个过滤器进行“卷积”时，我们将该信息与下一个卷积层相关联。例如 CNN 可以识别第一个卷积层中的基本边缘和颜色信息，然后 通过在第一层上卷积新过滤器，CNN 可以使用边缘和颜色信息，来归纳更复杂的结构 如车轮、车门和挡风玻璃。而另一个卷积可使用车轮、车门和挡风玻璃识别整个车辆。最后 神经网络可使用这一高阶信息对车辆进行分类。

人们通常不太清楚 CNN 如何解读图像，CNN 有时会侧重于图像中令人惊讶的部分，但这也是深度学习的神奇之处。**CNN 根据其任务查找真正需要的特征，任务可能是图像检测、分类、分割或其他类型的目标**。

## 检测与分类

在感知任务中,首先想到的是**障碍物检测和分类**。

在驾驶过程中会遇到许多障碍物，静态障碍物包括墙壁、树木、杆子和建筑物。动态障碍物包括行人、自行车和各种汽车。

**计算机首先需要知道这些障碍物的位置，然后对它们进行分类**。在路中行驶的无人驾驶车可能会探测到许多不同的物体，汽车根据所感知的物体类型，来确定路径和速度。如果感知到前方有一辆自行车，汽车可能会决定减速和变道 以便安全驶过自行车。但是 如果感知到前方有另一辆车，并预测到前方车辆也将以接近限速的速度行驶。无人驾驶车可能会保持其速度和车道。

另一个示例为 **交通信号灯检测分类**。首先 我们将使用计算机视觉对图像中的交通信号灯进行定位。然后 我们可以根据灯光显示颜色对交通信号灯进行分类。

![](http://p5ocy6pck.bkt.clouddn.com/%E5%88%86%E7%B1%BB.jpg)

在无人驾驶车辆中，我们使用什么算法来对障碍物进行检测和分类？我们可以先使用检测 CNN 来查找图像中的对象的位置，在对图像中的对象进行定位后，我们可以将图像发送给另一个 CNN 进行分类。

我们也可以使用单一 CNN 体系结构对对象进行检测和分类。一种通常的做法为在单个网络体系结构的末端附加几个不同的“头”，一个头可能执行检测，另一个则可能执行分类。

一个经典的体系结构为 R-CNN 及其变体 Fast R-CNN 和 Faster R-CNN、YOLO 和 SSD 是具有类似形式的不同体系结构。

## tracking跟踪

在检测完对象后 我们需要追踪它们。

追踪的意义是什么？如果我们对每个帧中的每个对象进行检测并用边界框对每个对象进行标识。

那么跨帧追踪对象会带来哪些好处？首先 追踪在检测失败时是至关重要的。如果你在运行检测算法时，对象被其他对象遮挡一部分，则检测算法可能会失败。追踪可以解决遮挡问题。另一个原因在于追踪可以保留身份。障碍物检测的输出为包含对象的边界框，但是 对象没有与任何身份关联，单独使用对象检测时，计算机不知道一个帧中的哪些对象与下一帧中的哪些对象相对应。

该任务对人类来说很简单 但对汽车来说很困难。追踪的第一步为确认身份，通过查找特征相似度最高的对象，我们将在之前的帧中检测到的所有对象与在当前的帧中检测到的对象进行匹配。对象具有各种特征，有些特征可能基于颜色 而另一些特征可能基于形状，计算机视觉算法可以计算出复杂的图像特征，如局部二值模式和方向梯度直方图。当然 我们也需要考虑连续视频帧中，两个障碍物之间的位置和速度。**由于两个帧之间的对象位置和速度没有太大变化，该信息也可以帮助我们快速找到匹配的对象**。在确定身份后 我们可以使用对象的位置，并结合预测算法以估计在下一个时间步的速度和位置，该预测可帮助我们识别下一帧中的相应对象。

## segmentation 语义分割

语义分割涉及对图像的每个像素进行分类。它用于尽可能详细地了解环境，并确定车辆可驾驶区域。

语义分割依赖于一种特殊类型的 CNN，它被称为全卷积网络 或 FCN。FCN 用卷积层来替代传统 CNN 体系结构末端的平坦层。现在 网络中的每一层都是卷积层，因此其名称为“全卷积网络”。

FCN 提供了可在原始输入图像之上叠加的逐像素输出，我们必须考虑的一个复杂因素是大小。在典型的 CNN 中 经过多次卷积之后，所产生的输出比原始输入图像小得多。

然而 为了分割像素，输出尺寸必须与原始图像的尺寸相匹配，为了达到该目的 我们可以对中间输出进行上采样处理，直到最终输出的大小与原始输出图像的大小相匹配，网络的前半部分通常被称为编码器。因为这部分网络对输入图像的特征进行了提取和编码，网络的后半部分通常被称为解码器，因为它对这些特征进行了解码 并将其应用于输出。

![](http://p5ocy6pck.bkt.clouddn.com/fully%20CN.jpg)

## Apollo 感知

Apollo 开放式软件栈可感知障碍物、交通信号灯和车道。对于三维对象检测，Apollo 在高精度地图上使用 感兴趣区域 (ROI)来重点关注相关对象。

Apollo 将 ROI 过滤器应用于点云和图像数据，以缩小搜索范围并加快感知。然后 通过检测网络馈送已过滤的点云，输出用于构建围绕对象的三维边界框，最后 我们使用被称为 **检测跟踪关联** 的算法来跨时间步识别单个对象。



该算法先保留在每个时间步要跟踪的对象列表，然后在下一个时间步中找到每个对象的最佳匹配。对于交通信号灯的分类,Apollo 先使用高精度地图来确定前方是否存在交通信号灯,如果前方有交通信号灯,则高精度地图会返回灯的位置,这侧重于摄像头搜索范围,在摄像头捕获到交通信号灯图像后,Apollo 使用检测网络对图像中的灯进行定位,然后 Apollo 从较大的图像中提取交通信号灯。Apollo 将裁剪的交通灯图像提供给分类网络，以确定灯颜色。如果有许多灯，则系统需要选择哪些灯与其车道相关。

![](http://p5ocy6pck.bkt.clouddn.com/detection%20network.jpg)

Apollo 使用 **YOLO 网络**，来检测车道线和动态物体。 其中包括车辆、卡车、骑自行车的人和行人，在经过 YOLO 网络检测后，在线检测模块会并入来自其他传感器的数据，对车道线预测进行调整，车道线最终被并入名为“虚拟车道”的单一数据结构中。同样 也通过其他传感器的数据对 YOLO 网络所检测到的动态对象进行调整以获得每个对象的类型、位置、速度和前进方向。虚拟通道和动态对象均被传递到规划与控制模块。

![](http://p5ocy6pck.bkt.clouddn.com/YOLO.jpg)

## 传感器数据比较
![](http://p5ocy6pck.bkt.clouddn.com/camera%20radar%20lidar%20compare.jpg)

**感知通常依赖于摄像头、激光雷达和雷达**。该图显示了这三种传感器的优缺点，绿色代表性能良好，黄色代表混合性能，红色代表性能不佳。

- 摄像头非常适用于分类，在 Apollo 中 摄像头主要用于交通信号灯分类 以及车道检测。
- 激光雷达的优势在于障碍物检测，即使在夜间 在没有自然光的情况下，激光雷达仍能准确地检测障碍物。
- 雷达在探测范围和应对恶劣天气方面占优势

通过融合这三种传感器的数据，可实现最佳聚合性能，这被称为“传感器融合”
### 雷达与激光雷达
雷达已经在汽车上使用很多年，在各种系统中都需要雷达，如自适应巡航控制、盲点警告、碰撞浸膏和碰撞预防系统等。尽管雷达技术已经成熟，它仍在不断进步，作用不断提升。其他传感器测量速度的方法是计算两次读数之间的差距，而雷达则通过多普勒效应来直接测量速度。多普勒效应根据对象在远离还是接近你，测量出雷达的频率变化。就像消防车警报器一样，当车辆正在远离你和驶向你时，听起来声是不一样的。多普勒效应对传感器融合至关重要。因为它可以把速度作为独立的测量参数，从而提升了融合算法的收敛速度。雷达还可以生成环境的雷达地图，进而实现定位。因为雷达波在坚硬表面会回弹。因此，它可以直接测量对象距离，无需在视线范围内也可以。雷达可以看到其他车辆底部。并发现可能会被阻挡的建筑物和对象。在车上的所有传感器中，雷达是至不容易受雨雾影响的。而且视野宽阔，可达 150 度，距离可达200 多米。与激光雷达和摄像头相比，雷达分辨率较低，尤其是在垂直方向，分辨率非常有限。分辨率低意味着来自静态物体的反射可能产生问题。例如，街道上检修孔盖或汽水罐，可能产生很高的雷达反射率，但他们并不大。我们将其称为雷达杂波。因此，当前的车载雷达通常会忽视静态物体。

激光雷达是激光探测与测量的简称，而雷达则谁无线电探测与测量的简称。雷达使用无线电波，而激光雷达则使用红激光束来确定传感器和附近对象的距离。目前的激光雷达大多使用 900 纳米光波长度的光源。但部分激光雷达使用的光波长度更长，在雨雾中性能更好。当前的激光雷达使用旋转座架发射激光，扫描周边环境。激光室脉冲式的，脉冲被对象反射，然后返回一个点云，来代表这些物体。激光雷达的空间分辨率远远高于雷达。因为激光束越聚焦，垂直方向的扫描层数量就越多，因此每层的激光雷达的密度也越高。目前，激光雷达还不能直接测量对象的速度，必须使用两次或多次扫描之间的位置差来确定。激光雷达受天气和传感器清洁程度影响也很大，因此需要保持清洁。它们块头也比其他传感器更大，因此也很难安装，除非你只想在车顶安装一个大的激光扫描器。

![](http://p5ocy6pck.bkt.clouddn.com/radar%20lidar.jpg)

## 感知融合策略

Apollo 使用激光雷达和雷达来检测障碍物。用于融合输出的主要算法为卡尔曼滤波。

![](http://p5ocy6pck.bkt.clouddn.com/two%20step%20estimation%20problem.jpg)

卡尔曼滤波有两个步骤。第一步为预测状态，第二步是更新测量结果。

设想我们正在跟踪一名行人，这里的状态表示行人的位置和速度，从已经掌握的行人状态开始。我们使用这些信息来执行卡尔曼滤波的第一步，即预测行人在将来的状态。

下一步为误差结果更新，我们使用新的传感器来更新我们所认为的行人状态，卡尔曼滤波算法是预测和更新步骤的无限循环。

![](http://p5ocy6pck.bkt.clouddn.com/fusion%20fuse.jpg)

实际上有两种测量结果更新步骤：同步和异步。同步融合同时更新来自不同传感器的测量结果，而异步融合则逐个更新所收到的传感器测量结果。传感器融合可提高感知性能 因为各传感器相辅相成，融合也可以减少跟踪误差，所以我们可以更加确信，对道路上其他物体位置的预测。
