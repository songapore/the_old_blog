---
layout: post
title:  "其他正则化方法（Other regularization methods）-吴恩达 深度学习 course2 1.8笔记"
date:   2018-04-17 14:50:00
categories: DeepLearning
tags: DeepLearning 吴恩达
mathjax: true
---

* content
{:toc}

除了L2正则化和随机失活（dropout）正则化，还有几种方法可以减少神经网络中的过拟合
<!--more-->



除了L2正则化和随机失活（dropout）正则化，还有几种方法可以减少神经网络中的过拟合:

## 数据扩增（Data Augmentation）
![Data Augmentation](http://p5ocy6pck.bkt.clouddn.com/%E6%95%B0%E6%8D%AE%E6%89%A9%E5%A2%9E.png)

假设你正在拟合猫咪图片分类器，如果你想通过扩增训练数据来解决过拟合，但扩增数据代价高，而且有时候我们无法扩增数据，但我们可以通过添加这类图片来增加训练集。例如，水平翻转图片，并把它添加到训练集。所以现在训练集中有原图，还有翻转后的这张图片，所以**通过水平翻转图片，训练集则可以增大一倍**，因为训练集有冗余，这虽然不如我们额外收集一组新图片那么好，但这样做节省了获取更多猫咪图片的花费。

除了水平翻转图片，你也可以**随意裁剪图片**，这张图是把原图旋转并随意放大后裁剪的，仍能辨别出图片中的猫咪。

**通过随意翻转和裁剪图片，我们可以增大数据集，额外生成假训练数据**。和全新的，独立的猫咪图片数据相比，这些额外的假的数据无法包含像全新数据那么多的信息，但我们这么做基本没有花费，代价几乎为零，除了一些对抗性代价。以这种方式扩增算法数据，进而正则化数据集，减少过拟合比较廉价。

对于字符识别，我们还可以通过添加数字，随意旋转或扭曲数字来扩增数据，把这些数字添加到训练集，它们仍然是数字。为了方便说明，我对字符做了强变形处理，所以数字4看起来是波形的，其实不用对数字4做这么夸张的扭曲，只要轻微的变形就好，我做成这样是为了让大家看的更清楚。实际操作的时候，我们通常对字符做更轻微的变形处理。因为这几个4看起来有点扭曲。所以，**数据扩增可作为正则化方法使用，实际功能上也与正则化相似**。

## 早停止法（Early Stopping）
![Early Stopping](http://p5ocy6pck.bkt.clouddn.com/early-stopping.png)

因为在训练过程中，我们希望训练误差，代价函数`J`都在下降，通过early stopping，我们不但可以绘制上面这些内容，还可以绘制验证集误差，它可以是验证集上的分类误差，或验证集上的代价函数，逻辑损失和对数损失等。你会发现，**验证集误差通常会先呈下降趋势，然后在某个节点处开始上升**。

early stopping的作用是，你会说，神经网络已经在这个迭代过程中表现得很好了，我们在此停止训练吧，得到验证集误差。

early stopping要做就是在中间点停止迭代过程，我们得到一个值中等大小的弗罗贝尼乌斯范数，与L2正则化相似，选择参数w范数较小的神经网络。

- early stopping的缺点：因为提早停止梯度下降，也就是停止了优化代价函数，所以代价函数的值可能不够小，同时你又希望不出现过拟合，你没有采取不同的方式来解决这两个问题，而是用一种方法同时解决两个问题，这样做的结果是我要考虑的东西变得更复杂。
- 如果不用early stopping，另一种方法就是正则化，训练神经网络的时间就可能很长。我发现，这导致超级参数搜索空间更容易分解，也更容易搜索，但是缺点在于，你必须尝试很多正则化参数的值，这也导致搜索大量值的计算代价太高
- Early stopping的优点是，只运行一次梯度下降，你可以找出的较小值，中间值和较大值，而L2正则化需尝试超级参数的很多值。
