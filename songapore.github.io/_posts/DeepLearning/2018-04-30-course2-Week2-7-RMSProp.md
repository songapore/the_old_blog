---
layout: post
title:  "RMSprop算法-吴恩达 深度学习 course2 2.7笔记"
date:   2018-04-30 12:40:12
categories: DeepLearning
tags: DeepLearning 吴恩达
mathjax: true
---

* content
{:toc}

RMSprop算法，全称是root mean square prop算法，它也可以加速梯度下降，我们来看看它是如何运作的。
<!--more-->



你们知道了动量（Momentum）可以加快梯度下降，还有一个叫做RMSprop的算法，全称是root mean square prop算法，它也可以加速梯度下降，我们来看看它是如何运作的。
## 原理
**RMSProp**算法是**在对梯度进行指数加权平均的基础上，引入平方和平方根**。

如果你执行梯度下降，虽然横轴方向正在推进，但纵轴方向会有大幅度摆动，为了分析这个例子，假设纵轴代表参数b，横轴代表参数W.

所以，你想减缓 纵轴方向的学习，同时加快 横轴方向的学习，RMSprop算法可以实现这一点

![RMSProp 算法](http://p5ocy6pck.bkt.clouddn.com/RMSProp.png)

我们来理解一下其原理。我们希望W学习速度快，而在垂直方向，也就是例子中的b方向，我们希望减缓纵轴上的摆动，所以有了和S_dw, S_db
.

我们希望S_dw会相对较小，所以我们要除以一个较小的数，而希望S_db又较大，所以这里我们要除以较大的数字，这样就可以减缓纵轴上的变化。

因为函数的倾斜程度，在纵轴上，也就是b方向上要大于在横轴上，也就是W方向上。db的平方较大，所以S_db也会较大，而相比之下，dw会小一些，亦或平方会小一些，因此S_dw会小一些，结果就是纵轴上的更新要被一个较大的数相除，就能消除摆动，而水平方向的更新则被较小的数相除。

RMSprop的影响就是你的更新最后会变成这样（绿色线），纵轴方向上摆动较小，而横轴方向继续推进。还有个影响就是，你可以用一个更大学习率，然后加快学习，而无须在纵轴上垂直方向偏离。
## 实现

![RMSProp 算法](http://p5ocy6pck.bkt.clouddn.com/RMSProp%20%281%29.png)
$$
s_{dw} = \beta s_{dw} + (1 - \beta)(dw)^2
$$

$$
s_{db} = \beta s_{db} + (1 - \beta)(db)^2
$$

$$
w := w - \alpha \frac{dw}{\sqrt{s_{dw} + \epsilon}}
$$

$$
b := b - \alpha \frac{db}{\sqrt{s_{db} + \epsilon}}
$$


如果的平方根趋近于0怎么办？得到的答案就非常大，为了确保数值稳定，在实际中，你要在分母上加上一个很小很小的ϵ，是多少没关系，10^-8
是个不错的选择，这只是保证数值能稳定一些。
## 题外话
所以RMSprop跟Momentum有很相似的一点，可以消除梯度下降中的摆动，包括mini-batch梯度下降，并允许你使用一个更大的学习率，从而加快你的算法学习速度。

所以你学会了如何运用RMSprop，这是给学习算法加速的另一方法。关于RMSprop的一个有趣的事是，它首次提出并不是在学术研究论文中，而是在多年前Jeff Hinton在Coursera的课程上。我想Coursera并不是故意打算成为一个传播新兴的学术研究的平台，但是却达到了意想不到的效果。就是从Coursera课程开始，RMSprop开始被人们广为熟知，并且发展迅猛。

我们讲过了Momentum，我们讲了RMSprop，如果二者结合起来，你会得到一个更好的优化算法，在下个视频中我们再好好讲一讲为什么。
