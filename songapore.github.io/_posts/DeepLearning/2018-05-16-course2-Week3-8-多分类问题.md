---
layout: post
title:  "Softmax 回归（Softmax regression）-吴恩达 深度学习 course2 3.8~3.9笔记"
date:   2018-05-12 19:51:31
categories: DeepLearning
tags: DeepLearning 吴恩达
mathjax: true
---
* content
{:toc}

有一种 Logistic 回归的一般形式，叫做 **Softmax 回归**，可以处理多分类问题。
<!--more-->


# 3.8 Softmax 回归（Softmax regression）
目前为止，介绍的分类例子都是二分类问题：神经网络输出层只有一个神经元，表示预测输出 y^是正类的概率 P(y = 1|x)，y > 0.5 则判断为正类，反之判断为负类。

对于**多分类问题**，用 C 表示种类个数，则神经网络输出层，也就是第 L 层的单元数量 n[L]=C。每个神经元的输出依次对应属于该类的概率，即 P(y=c|x),c=0,1,..,C−1。有一种 Logistic 回归的一般形式，叫做 **Softmax 回归**，可以处理多分类问题。

对于 Softmax 回归模型的输出层，即第 L 层，有：


$$
Z^{[L]} = W^{[L]}a^{[L-1]} + b^{[L]}
$$


$$
a^{[L]}_i = \frac{e^{Z^{[L]}_i}}{\sum^C_{i=1}e^{Z^{[L]}_i}}
$$

为输出层每个神经元的输出，对应属于该类的概率，满足：

$$
\sum^C_{i=1}a^{[L]}_i = 1
$$
简单来说就是用临时变量t将它归一化，使总和为1。一个直观的计算例子如下：

![softmax layer](http://p5ocy6pck.bkt.clouddn.com/softmax%20layer.png)

# 3.9 训练一个 Softmax 分类器（Training a Softmax classifier）

- 怎样训练带有Softmax输出层的神经网络，具体而言，我们先定义训练神经网络使会用到的损失函数。


$$
L(\hat y, y) = -\sum^C_{j=1}y_jlog\hat y_j
$$
概括来讲，损失函数所做的就是它找到你的训练集中的真实类别，然后试图使该类别相应的概率尽可能地高，如果你熟悉统计学中最大似然估计，这其实就是最大似然估计的一种形式。

- 这是单个训练样本的损失，整个训练集的损失J又如何呢？


$$
J = \frac{1}{m}\sum^m_{i=1}L(\hat y, y)
$$
因此你要做的就是用梯度下降法，使这里的损失最小化。

- 最后我们来看一下，在有Softmax输出层时如何实现梯度下降法

多分类的 Softmax 回归模型与二分类的 Logistic 回归模型只有输出层上有一点区别。经过不太一样的推导过程，仍有


$$
dZ^{[L]} = A^{[L]} - Y
$$
反向传播过程的其他步骤也和 Logistic 回归的一致。

详细可参考：[softmax 回归](http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92)
