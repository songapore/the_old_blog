---
layout: post
title:  "Mini-batch 梯度下降（Mini-batch gradient descent）-吴恩达 深度学习 course2 2.1~2.2笔记"
date:   2018-04-27 20:06:10
categories: DeepLearning
tags: DeepLearning 吴恩达
mathjax: true
---

* content
{:toc}

Mini-batch 梯度下降（Mini-batch gradient descent）
<!--more-->


# Mini-batch 梯度下降（Mini-batch gradient descent）
## 作用
机器学习的应用是一个高度依赖经验的过程，伴随着大量迭代的过程，你需要训练诸多模型，才能找到合适的那一个，而优化算法能够帮助你快速训练模型。

深度学习难以在大数据领域发挥最大效果的一个原因是，在巨大的数据集基础上进行训练速度很慢。而优化算法能够帮助快速训练模型，大大提高效率。
## 定义
- **batch 梯度下降法**（批梯度下降法，我们之前一直使用的梯度下降法）是最常用的梯度下降形式，即同时处理整个训练集。其在更新参数时使用所有的样本来进行更新

但是如果每次处理训练数据的一部分即进行梯度下降法，则我们的算法速度会执行的更快。而处理的这些一小部分训练子集即称为 mini-batch。

- **mini-batch梯度下降法**每次同时处理单个的 mini-batch，其他与 batch 梯度下降法一致。

## 工作原理
那么究竟mini-batch梯度下降法的原理是什么？在训练集上运行mini-batch梯度下降法，你运行for t=1……5000，因为我们有5000个各有1000个样本的组，在for循环里你要做得基本就是对 X^{t}和Y^{t} 执行一步梯度下降法。假设你有一个拥有1000个样本的训练集，而且假设你已经很熟悉一次性处理完的方法，你要用向量化去几乎同时处理1000个样本。

![Mini-batch gradient descent](http://p5ocy6pck.bkt.clouddn.com/Mini-batch%20gradient%20descent.png)

你也会注意到，我们做的一切似曾相识，其实跟之前我们执行梯度下降法如出一辙，除了你现在的对象不是X，Y，而是 X^{t} 和 Y^{t}

这是使用mini-batch梯度下降法训练样本的一步，我写下的代码也可被称为进行“一代”（1 epoch）的训练。一代这个词意味着只是一次遍历了训练集。

使用batch梯度下降法，一次遍历训练集只能让你做一个梯度下降，使用mini-batch梯度下降法，一次遍历训练集，能让你做5000个梯度下降。当然正常来说你想要多次遍历训练集，还需要为另一个while循环设置另一个for循环。所以你可以一直处理遍历训练集，直到最后你能收敛到一个合适的精度。

如果你有一个丢失的训练集，mini-batch梯度下降法比batch梯度下降法运行地更快，所以几乎每个研习深度学习的人在训练巨大的数据集时都会用到，下一课中，我们将进一步深度讨论mini-batch梯度下降法，你也会因此更好地理解它的作用和原理。

# 理解mini-batch梯度下降法
我们将进一步学习如何执行梯度下降法，更好地理解其作用和原理。
## 原理
Mini-Batch 梯度下降法（小批量梯度下降法）每次同时处理单个的 mini-batch，其他内容与 batch 梯度下降法一致。

使用 batch 梯度下降法，对整个训练集的一次遍历只能做一个梯度下降；而使用 Mini-Batch 梯度下降法，对整个训练集的一次遍历（称为一个 epoch）能做 mini-batch 个数个梯度下降。之后，可以一直遍历训练集，直到最后收敛到一个合适的精度。

batch 梯度下降法和 Mini-batch 梯度下降法代价函数的变化趋势如下：

![training-with-mini-batch-gradient-descent](http://p5ocy6pck.bkt.clouddn.com/training-with-mini-batch-gradient-descent.png)

使用batch梯度下降法时，每次迭代你都需要历遍整个训练集，可以预期每次迭代成本都会下降，所以成本函数J是迭代次数的一个函数，它应该会随着每次迭代而减少，如果在某次迭代中J增加了，那肯定出了问题，也许你的学习率太大。

使用mini-batch梯度下降法，如果你作出成本函数在整个过程中的图，发现并不是每次迭代都是下降的，特别是在每次迭代中，你要处理的是 X^{t} 和 Y^{t}，如果要作出成本函数 J^{t} 的图，而 J^{t} 只和 X^{t} 和 Y^{t} 有关，也就是每次迭代下你都在训练不同的样本集或者说训练不同的mini-batch，如果你要作出成本函数的图，你很可能会看到这样的结果，走向朝下，但有更多的噪声，所以如果你作出的J^{t}图，因为在训练mini-batch梯度下降法时，会经过多代，你可能会看到这样的曲线。没有每次迭代都下降是不要紧的，但走势应该向下。

## batch 的不同大小（size）带来的影响
- mini-batch 的大小为 1，即是**随机梯度下降法**（stochastic gradient descent），每个样本都是独立的 mini-batch；
- mini-batch 的大小为 m（数据集大小），即是 batch 梯度下降法；

![choosing your mini_batch size](http://p5ocy6pck.bkt.clouddn.com/choosing-mini-batch-size.png)

- **batch 梯度下降法**：

1. 对所有 m 个训练样本执行一次梯度下降，每一次迭代时间较长，训练过程慢；
1. 相对噪声低一些，幅度也大一些；
1. 成本函数总是向减小的方向下降。
- **随机梯度下降法**：

1. 对每一个训练样本执行一次梯度下降，训练速度快，但丢失了向量化带来的计算加速；
1. 有很多噪声，减小学习率可以适当；
1. 成本函数总体趋势向全局最小值靠近，但永远不会收敛，而是一直在最小值附近波动。
## mini-batch 大小的选择
如果mini-batch大小既不是1也不是，应该取中间值，那应该怎么选择呢？其实是有指导原则的。

首先，如果训练集较小，直接使用batch梯度下降法，样本集较小就没必要使用mini-batch梯度下降法，你可以快速处理整个训练集，所以使用batch梯度下降法也很好，这里的少是说 **小于2000个样本，** 这样比较适合使用batch梯度下降法。

不然，样本数目较大的话，**一般的mini-batch大小为64到512**， **考虑到电脑内存设置和使用的方式，如果mini-batch大小是2的
n
次方，** 代码会运行地快一些，64就是2的6次方，以此类推，128是2的7次方，256是2的8次方，512是2的9次方。所以我经常把mini-batch大小设成2的次方。在上一个视频里，我的mini-batch大小设为了1000，建议你可以试一下1024，也就是2的10次方。也有mini-batch的大小为1024，不过比较少见，64到512的mini-batch比较常见。

最后需要注意的是在你的mini-batch中，要确保和要符合CPU/GPU内存，取决于你的应用方向以及训练集的大小。如果你处理的mini-batch和CPU/GPU内存不相符，不管你用什么方法处理数据，你会注意到算法的表现急转直下变得惨不忍睹，所以我希望你对一般人们使用的mini-batch大小有一个直观了解。事实上mini-batch大小是另一个重要的变量，你需要做一个快速尝试，才能找到能够最有效地减少成本函数的那个，我一般会尝试几个不同的值，几个不同的2次方，然后看能否找到一个让梯度下降优化算法最高效的大小。希望这些能够指导你如何开始找到这一数值。

总结起来，
- 如果训练样本的大小比较小，如 m ⩽ 2000 时，选择 batch 梯度下降法；
- 如果训练样本的大小比较大，选择 Mini-Batch 梯度下降法。为了和计算机的信息存储方式相适应，代码在 mini-batch 大小为 2 的幂次时运行要快一些。典型的大小为 26、27、...、29；
- mini-batch 的大小要符合 CPU/GPU 内存。
- mini-batch 的大小也是一个重要的超变量，需要根据经验快速尝试，找到能够最有效地减少成本函数的值。

## 获得 mini-batch 的步骤
- 将数据集打乱；
- 按照既定的大小分割数据集；
其中打乱数据集的代码：


```
m = X.shape[1]
permutation = list(np.random.permutation(m))
shuffled_X = X[:, permutation]
shuffled_Y = Y[:, permutation].reshape((1,m))
```

- **符号表示约定**
- 使用上角小括号 i 表示训练集里的值，x^(i) 是第 i 个训练样本；
- 使用上角中括号 l 表示神经网络的层数，z^[l] 表示神经网络中第 l 层的 z 值；
- 现在引入大括号 t 来代表不同的 mini-batch，因此有 X^{t} 、Y^{t}
