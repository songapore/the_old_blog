---
layout: post
title:  "正则化（Regularization）-吴恩达 深度学习 course2 1.4笔记"
date:   2018-04-17 14:36:00
categories: DeepLearning
tags: DeepLearning 吴恩达
use_math: true
---

* content
{:toc}

深度学习可能存在过拟合问题——高方差，有两个解决方法，一个是正则化，另一个是准备更多的数据，这是非常可靠的方法，但你可能无法时时刻刻准备足够多的训练数据或者获取更多数据的成本很高，但正则化通常有助于避免过拟合，或减少你的网络误差。
<!--more-->


## 正则化避免过拟合
深度学习可能存在**过拟合问题——高方差**，有两个解决方法，一个是**正则化**，另一个是**准备更多的数据**，这是非常可靠的方法，但你可能无法时时刻刻准备足够多的训练数据或者获取更多数据的成本很高，但**正则化通常有助于避免过拟合，或减少你的网络误差**。

## Logistic 回归中的正则化

- 在逻辑回归函数中加入正则化，只需添加参数λ，也就是正则化参数。
加入 L2 正则化（也称“L2 范数”）的成本函数为


$$
J(w,b) = \frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2m}{||w||}^2_2
$$

- L2 正则化：

$$
\frac{\lambda}{2m}{||w||}^2_2 = \frac{\lambda}{2m}\sum_{j=1}^{n_x}w^2_j = \frac{\lambda}{2m}w^Tw
$$

- L1 正则化：

$$
\frac{\lambda}{2m}{||w||}_1 = \frac{\lambda}{2m}\sum_{j=1}^{n_x}{|w_j|}
$$

其中，**λ 为正则化因子，是超参数**。

由于 L1 正则化最后得到 w 向量中将存在大量的 0，使模型变得稀疏化，因此 **人们在训练网络时，越来越倾向于使用L2 正则化**。

注意，lambda在 Python 中属于保留字，所以在编程的时候，用lambd代替这里的正则化因子。

这就是在逻辑回归函数中实现正则化的过程
![Logistic 回归中的正则化](http://p5ocy6pck.bkt.clouddn.com/Logistic%20%E5%9B%9E%E5%BD%92%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96.png)
## 神经网络中的正则化

![神经网络中的正则化](http://p5ocy6pck.bkt.clouddn.com/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96.png)

神经网络中加入正则化的成本函数为

$$
J(w^{[1]}, b^{[1]}, ..., w^{[L]}, b^{[L]}) = \frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2m}\sum_{l=1}^L{||w^{[l]}||}^2_F
$$

字母 L 是神经网络所含的层数。因为是一个
$$
n^{[l]}\times n^{[l-1]}
$$
的多维矩阵,有：

$$
{||w^{[l]}||}^2_F = \sum^{n^{[l-1]}}_{i=1}\sum^{n^{[l]}}_{j=1}(w^{[l]}_{ij})^2
$$

该矩阵范数被称作“**弗罗贝尼乌斯范数（Frobenius Norm）**”，用下标 F 标注.

## 如何使用该范数实现梯度下降呢？

backprop计算出 *dw* 的值。在加入正则化项后，梯度变为

$$
dW^{[l]}= \frac{\partial L}{\partial w^{[l]}} +\frac{\lambda}{m}W^{[l]}
$$

代入梯度更新公式：

$$
W^{[l]} := W^{[l]}-\alpha dW^{[l]}
$$

可得：

$$
W^{[l]} := W^{[l]} - \alpha [\frac{\partial L}{\partial w^{[l]}} + \frac{\lambda}{m}W^{[l]}]
$$

$$
= W^{[l]} - \alpha \frac{\lambda}{m}W^{[l]} - \alpha \frac{\partial L}{\partial w^{[l]}}
$$

$$
= (1 - \frac{\alpha\lambda}{m})W^{[l]} - \alpha \frac{\partial L}{\partial w^{[l]}}
$$

其中，因为 1−αλ/m<1，会给原来的 W[l]一个衰减的参数，因此 L2 正则化项也被称为**权重衰减（Weight Decay）**。
