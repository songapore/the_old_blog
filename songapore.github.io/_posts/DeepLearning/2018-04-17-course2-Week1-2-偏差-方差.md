---
layout: post
title:  "偏差Bias 方差Variance-吴恩达 深度学习 course2 1.2笔记"
date:   2018-04-17 14:30:00
categories: DeepLearning
tags: DeepLearning 吴恩达
mathjax: true
---

* content
{:toc}

泛化误差可分解为偏差、方差与噪声之和（Error = Bias + Variance + Noise）
<!--more-->

## 泛化误差
**泛化误差**可分解为偏差、方差与噪声之和（Error = Bias + Variance + Noise）：
- 偏差：度量了学习算法的期望预测与真实结果的偏离程度，即模型本身的精准度，反应出算法的拟合能力。
- 方差：度量了同样大小的训练集的变动所导致的学习性能的变化，即模型的稳定性，反应出预测的波动情况。
- 噪声：表达了在当前任务上任何学习算法所能够达到的期望泛化误差的下界，即刻画了学习问题本身的难度。

## 偏差与方差的关系
![偏差与方差的关系](http://p5ocy6pck.bkt.clouddn.com/%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE.jpg)
偏差与方差之间按照高低，可以组合成四种关系:

- 低偏差 高方差

表示模型准确但是稳定性差，对验证数据&测试数据的拟合能力差，即是模型的泛化能力差，产生了**过拟合(Overfitting)**。

- 高偏差 低方差

表示模型的准确度差，对数据的拟合能力弱，产生了**欠拟合(Underfitting)**。

- 高偏差高方差

表示模型既不准确又不稳定。
- 低偏差低方差

表示模型既准确又稳定，效果最好，但是现实中这种情形很少遇见。
## 过拟合与欠拟合
由上面的分析可知，**高方差往往预示着过拟合，高偏差则是欠拟合**。

### 避免欠拟合(拟合太差)

1、增加训练样本数据

2、设计更复杂的神经网络模型

3、增加迭代次数

4、更好的优化函数

5、调整超参数值

### 避免过拟合(拟合过度，泛化太差)

1、设计更简单的神经网络模型

2、增加训练样本数据

3、正则化。在损失函数后面添加上L2正则项

4、使用dropout。随机性使得网络中的部分神经元失效，效果上类似将模型变得更简单。

5、调整超参数值

6、尝试其他模型

7、提前结束训练(early stopping)。即是提前结束优化损失函数。

## 简单小结

在实际工程中，通常可以按下面的来操作

**贝叶斯(最优)误差** ：  理论上的最小误差值(通常比人类误差小)

**可避免偏差** ：训练误差 与 贝叶斯误差 之间的差值

**方差**：验证集误差 与 训练误差 的差值

- 当 可避免偏差 大于 方差 时，发生 欠拟合。

- 当 方差 大于 可避免偏差 时，发生 过拟合。

在训练模型时对照以上描述，有助于定位问题，更快找到最适合的模型。
