---
layout: post
title:  "动量梯度下降法（Gradient descent with Momentum）-吴恩达 深度学习 course2 2.6笔记"
date:   2018-04-29 19:57:23
categories: DeepLearning
tags: DeepLearning 吴恩达
mathjax: true
---

* content
{:toc}

动量梯度下降（Gradient Descent with Momentum）是计算梯度的指数加权平均数，并利用该值来更新参数值
<!--more-->


## 定义及实现过程
**动量梯度下降**（Gradient Descent with Momentum）是计算梯度的指数加权平均数，并利用该值来更新参数值。

动量梯度下降法的运行速度几乎总是快于标准的梯度下降算法。具体过程为：

for l = 1, .. , L：


$$
v_{dW^{[l]}} = \beta v_{dW^{[l]}} + (1 - \beta) dW^{[l]}
$$

$$
v_{db^{[l]}} = \beta v_{db^{[l]}} + (1 - \beta) db^{[l]}
$$

$$
W^{[l]} := W^{[l]} - \alpha v_{dW^{[l]}}
$$

$$
b^{[l]} := b^{[l]} - \alpha v_{db^{[l]}}
$$

其中，将动量衰减参数 β 设置为 0.9 是超参数的一个常见且效果不错的选择。当 β 被设置为 0 时，显然就成了 batch 梯度下降法。

![Gradient-Descent-with-Momentum](http://p5ocy6pck.bkt.clouddn.com/Gradient-Descent-with-Momentum.png)

进行一般的梯度下降将会得到图中的蓝色曲线，由于存在上下波动，减缓了梯度下降的速度，因此只能使用一个较小的学习率进行迭代。如果用较大的学习率，结果可能会像紫色曲线一样偏离函数的范围。

而使用动量梯度下降时，通过累加过去的梯度值来减少抵达最小值路径上的波动，加速了收敛，因此在横轴方向下降得更快，从而得到图中红色的曲线。

当前后梯度方向一致时，动量梯度下降能够加速学习；而前后梯度方向不一致时，动量梯度下降能够抑制震荡。

另外，在 10 次迭代之后，移动平均已经不再是一个具有偏差的预测。因此实际在使用梯度下降法或者动量梯度下降法时，不会同时进行偏差修正。

## 动量梯度下降法的形象解释
将成本函数想象为一个碗状，从顶部开始运动的小球向下滚，其中 dw，db 想象成球的加速度；而 vdw、vdb 相当于速度。

小球在向下滚动的过程中，因为加速度的存在速度会变快，但是由于 β 的存在，其值小于 1，可以认为是摩擦力，所以球不会无限加速下去。

最后要说一点，如果你查阅了动量梯度下降法相关资料，你经常会看到 1-β 被删除了，即

$$
v_{dW^{[l]}} = \beta v_{dW^{[l]}} +dW^{[l]}
$$

所以V_dw缩小了1-β倍,所以你要用梯度下降最新值的话，a也要相应变化。实际上这2种方法效果都不错，只会影响到学习率a的最佳值。
