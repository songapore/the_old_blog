---
layout: post
title:  "梯度消失 梯度爆炸（Vanishing/ Exploding gradients）-吴恩达 深度学习 course2 1.10笔记"
date:   2018-04-19 20:20:10
categories: DeepLearning
tags: DeepLearning 吴恩达
mathjax: true
---

* content
{:toc}

训练神经网络，尤其是深度神经所面临的一个问题就是梯度消失或梯度爆炸，也就是你训练神经网络的时候，导数或坡度有时会变得非常大，或者非常小，甚至于以指数方式变小，这加大了训练的难度。
<!--more-->



## 定义
训练神经网络，尤其是深度神经所面临的一个问题就是梯度消失或梯度爆炸，也就是你训练神经网络的时候，导数或坡度有时会变得非常大，或者非常小，甚至于以指数方式变小，这加大了训练的难度。

![Vanishing Exploding gradients](http://p5ocy6pck.bkt.clouddn.com/Vanishing%20_Exploding%20gradients.png)

在深度神经网络中，激活函数以与L相关的指数级数增长或下降的情况,分别称为**梯度爆炸或者梯度消失**。与层数L相关的导数或梯度函数，也是呈指数级增长或呈指数递减。

## 发生梯度消失、梯度爆炸的原因
为了简单起见，假设我们使用激活函数

$$
g(z) = z, b^{[l]} = 0
$$

也就是线性激活函数,且忽略了b。对于输出有：

$$
\hat{y} = W^{[L]}W^{[L-1]}...W^{[2]}W^{[1]}X
$$

- 对于 W[l]的值大于 1 的情况，激活函数的值将以指数级递增；
- 对于 W[l]的值小于 1 的情况，激活函数的值将以指数级递减。
- 虽然我只是讨论了激活函数以与L相关的指数级数增长或下降，它也适用于与层数L相关的导数或梯度函数，也是呈指数级增长或呈指数递减。


最近Microsoft对152层神经网络的研究取得了很大进展，在这样一个深度神经网络中，如果激活函数或梯度函数以与相关的指数增长或递减，它们的值将会变得极大或极小，从而导致训练难度上升，尤其是梯度指数小于时，梯度下降算法的步长会非常非常小，梯度下降算法将花费很长时间来学习。
