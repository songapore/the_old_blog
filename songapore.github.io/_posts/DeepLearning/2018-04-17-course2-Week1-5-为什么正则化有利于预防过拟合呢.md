---
layout: post
title:  "为什么正则化有利于预防过拟合呢-吴恩达 深度学习 course2 1.5笔记"
date:   2018-04-17 14:40:00
categories: DeepLearning
tags: DeepLearning 吴恩达
mathjax: true
---

* content
{:toc}

如果正则化参数`λ`变得很大，参数`W`很小，`z`也会相对变小，此时`b`忽略的影响，`z`会相对变小，实际上，`z`的取值范围很小，这个激活函数，也就是曲线函数会相对呈线性，整个神经网络会计算离线性函数近的值，这个线性函数非常简单，并不是一个极复杂的高度非线性函数，不会发生过拟合
<!--more-->



## 直观理解
直观上理解就是如果正则化`λ`设置得足够大，权重矩阵`W`被设置为接近于0的值，直观理解就是把多隐藏单元的权重设为0，于是基本上消除了这些隐藏单元的许多影响。如果是这种情况，这个被大大简化了的神经网络会变成一个很小的网络，小到如同一个逻辑回归单元，可是深度却很大。
![regularization_prevent_overfitting](http://p5ocy6pck.bkt.clouddn.com/regularization_prevent_overfitting0.png)


## 数学解释
我们再来直观感受一下，正则化为什么可以预防过拟合，假设我们用的是这样的双曲线激活函数
`g(z) = tanh(z)`（sigmoid 同理）
![regularization_prevent_overfitting](http://p5ocy6pck.bkt.clouddn.com/regularization_prevent_overfitting.png)

总结一下，如果正则化参数`λ`变得很大，参数`W`很小，`z`也会相对变小，此时`b`忽略的影响，`z`会相对变小，实际上，`z`的取值范围很小，这个激活函数，也就是曲线函数会相对呈线性，整个神经网络会计算离线性函数近的值，这个线性函数非常简单，并不是一个极复杂的高度非线性函数，不会发生过拟合。
